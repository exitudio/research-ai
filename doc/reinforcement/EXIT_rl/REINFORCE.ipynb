{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[reference](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb) <br>\n",
    "[pytorch ](https://pytorch.org/docs/stable/distributions.html) <br>\n",
    "[1st rank cartpole](https://github.com/udacity/deep-reinforcement-learning/blob/master/hill-climbing/Hill_Climbing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from EXITrl.table_base import TableBase\n",
    "from EXITrl.approx_v_base import ApproxVBase\n",
    "from EXITrl.approx_policy_base import ApproxPolicyBase\n",
    "from gridworld_env_2d_state import GridworldEnv2DState\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode 10\tAverage Score: 20.80 \tother{}\n",
      "Episode 20\tAverage Score: 19.00 \tother{}\n",
      "Episode 30\tAverage Score: 34.20 \tother{}\n",
      "Episode 40\tAverage Score: 26.80 \tother{}\n",
      "Episode 50\tAverage Score: 24.90 \tother{}\n",
      "Episode 60\tAverage Score: 32.90 \tother{}\n",
      "Episode 70\tAverage Score: 39.30 \tother{}\n",
      "Episode 80\tAverage Score: 58.50 \tother{}\n",
      "Episode 90\tAverage Score: 60.10 \tother{}\n",
      "Episode 100\tAverage Score: 45.70 \tother{}\n",
      "Episode 110\tAverage Score: 54.40 \tother{}\n",
      "Episode 120\tAverage Score: 51.70 \tother{}\n",
      "Episode 130\tAverage Score: 56.50 \tother{}\n",
      "Episode 140\tAverage Score: 61.40 \tother{}\n",
      "Episode 150\tAverage Score: 71.90 \tother{}\n",
      "Episode 160\tAverage Score: 70.50 \tother{}\n",
      "Episode 170\tAverage Score: 122.10 \tother{}\n",
      "Episode 180\tAverage Score: 136.10 \tother{}\n",
      "Episode 190\tAverage Score: 164.10 \tother{}\n",
      "Episode 200\tAverage Score: 134.10 \tother{}\n",
      "Episode 210\tAverage Score: 179.30 \tother{}\n",
      "Episode 220\tAverage Score: 286.10 \tother{}\n",
      "Episode 230\tAverage Score: 275.10 \tother{}\n",
      "Episode 240\tAverage Score: 231.00 \tother{}\n",
      "Episode 250\tAverage Score: 197.40 \tother{}\n",
      "Episode 260\tAverage Score: 170.70 \tother{}\n",
      "Episode 270\tAverage Score: 346.50 \tother{}\n",
      "Episode 280\tAverage Score: 363.40 \tother{}\n",
      "Episode 290\tAverage Score: 287.70 \tother{}\n",
      "Episode 300\tAverage Score: 406.00 \tother{}\n",
      "Episode 310\tAverage Score: 441.80 \tother{}\n",
      "Episode 320\tAverage Score: 421.70 \tother{}\n",
      "Episode 330\tAverage Score: 412.70 \tother{}\n",
      "Episode 340\tAverage Score: 469.00 \tother{}\n",
      "Episode 350\tAverage Score: 464.20 \tother{}\n",
      "Episode 360\tAverage Score: 463.20 \tother{}\n",
      "Episode 370\tAverage Score: 479.50 \tother{}\n",
      "Episode 380\tAverage Score: 385.60 \tother{}\n",
      "Episode 390\tAverage Score: 448.70 \tother{}\n",
      "Episode 400\tAverage Score: 411.60 \tother{}\n",
      "Episode 410\tAverage Score: 260.70 \tother{}\n",
      "Episode 420\tAverage Score: 334.30 \tother{}\n",
      "Episode 430\tAverage Score: 483.00 \tother{}\n",
      "Episode 440\tAverage Score: 326.80 \tother{}\n",
      "Episode 450\tAverage Score: 410.90 \tother{}\n",
      "Episode 460\tAverage Score: 447.30 \tother{}\n",
      "Episode 470\tAverage Score: 298.70 \tother{}\n",
      "Episode 480\tAverage Score: 201.50 \tother{}\n",
      "Episode 490\tAverage Score: 249.10 \tother{}\n",
      "Episode 500\tAverage Score: 474.50 \tother{}\n"
     ]
    }
   ],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "class REINFORCE(ApproxPolicyBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.initialize()\n",
    "    \n",
    "    def _loop(self, episode) -> int:\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        trajectory = []\n",
    "        for i in range(1000):\n",
    "            action, log_prob = self.policy(state)\n",
    "            _state, reward, done, _ = self.env.step(action)\n",
    "            trajectory.append((state, reward, done, log_prob))\n",
    "            total_reward += reward\n",
    "            state = _state\n",
    "            if done: break\n",
    "                \n",
    "        loss = []\n",
    "        for t in range(len(trajectory)):\n",
    "            G = sum([self.gamma**k * reward for k, (state, reward, done, _) in enumerate(trajectory[t:])])\n",
    "            _, _, _, log_prob = trajectory[t]\n",
    "            # from Sutton's book said multiply by gamma**t \n",
    "            # but from David course no need too \n",
    "            # I tried both work\n",
    "            loss.append(self.gamma**t * G * (-log_prob))\n",
    "        self.update_policy(torch.stack(loss).sum())\n",
    "        return total_reward\n",
    "    \n",
    "s = REINFORCE(env, \n",
    "               num_episodes=500,\n",
    "               policy=\"softmax_policy\",\n",
    "               alpha=0.007, \n",
    "               gamma=.99)\n",
    "s.train(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode 10\tAverage Score: 18.00 \tother{}\n",
      "Episode 20\tAverage Score: 10.80 \tother{}\n",
      "Episode 30\tAverage Score: 9.50 \tother{}}\n",
      "Episode 40\tAverage Score: 9.60 \tother{}\n",
      "Episode 50\tAverage Score: 9.20 \tother{}\n",
      "Episode 60\tAverage Score: 9.10 \tother{}\n",
      "Episode 70\tAverage Score: 9.20 \tother{}\n",
      "Episode 80\tAverage Score: 9.40 \tother{}\n",
      "Episode 90\tAverage Score: 9.50 \tother{}\n",
      "Episode 100\tAverage Score: 9.20 \tother{}\n",
      "Episode 110\tAverage Score: 19.20 \tother{}\n",
      "Episode 120\tAverage Score: 53.40 \tother{}\n",
      "Episode 130\tAverage Score: 14.00 \tother{}\n",
      "Episode 140\tAverage Score: 9.60 \tother{}}\n",
      "Episode 150\tAverage Score: 9.20 \tother{}\n",
      "Episode 160\tAverage Score: 9.50 \tother{}\n",
      "Episode 170\tAverage Score: 9.50 \tother{}\n",
      "Episode 180\tAverage Score: 9.40 \tother{}\n",
      "Episode 190\tAverage Score: 9.30 \tother{}\n",
      "Episode 200\tAverage Score: 8.80 \tother{}\n",
      "Episode 210\tAverage Score: 9.10 \tother{}\n",
      "Episode 220\tAverage Score: 8.80 \tother{}\n",
      "Episode 230\tAverage Score: 9.70 \tother{}\n",
      "Episode 240\tAverage Score: 9.60 \tother{}\n",
      "Episode 250\tAverage Score: 9.40 \tother{}\n",
      "Episode 260\tAverage Score: 9.70 \tother{}\n",
      "Episode 270\tAverage Score: 9.10 \tother{}\n",
      "Episode 280\tAverage Score: 9.30 \tother{}\n",
      "Episode 290\tAverage Score: 9.20 \tother{}\n",
      "Episode 300\tAverage Score: 9.60 \tother{}\n",
      "Episode 310\tAverage Score: 9.50 \tother{}\n",
      "Episode 320\tAverage Score: 9.80 \tother{}\n",
      "Episode 330\tAverage Score: 9.20 \tother{}\n",
      "Episode 340\tAverage Score: 9.90 \tother{}\n",
      "Episode 350\tAverage Score: 9.70 \tother{}\n",
      "Episode 360\tAverage Score: 9.30 \tother{}\n",
      "Episode 370\tAverage Score: 9.40 \tother{}\n",
      "Episode 380\tAverage Score: 9.60 \tother{}\n",
      "Episode 390\tAverage Score: 9.30 \tother{}\n",
      "Episode 400\tAverage Score: 9.80 \tother{}\n",
      "Episode 410\tAverage Score: 9.00 \tother{}\n",
      "Episode 420\tAverage Score: 9.30 \tother{}\n",
      "Episode 430\tAverage Score: 9.20 \tother{}\n",
      "Episode 440\tAverage Score: 9.20 \tother{}\n",
      "Episode 450\tAverage Score: 9.30 \tother{}\n",
      "Episode 460\tAverage Score: 9.40 \tother{}\n",
      "Episode 470\tAverage Score: 9.70 \tother{}\n",
      "Episode 480\tAverage Score: 9.50 \tother{}\n",
      "Episode 490\tAverage Score: 9.00 \tother{}\n",
      "Episode 500\tAverage Score: 9.70 \tother{}\n"
     ]
    }
   ],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "class REINFORCE_with_baseline(ApproxVBase, ApproxPolicyBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        hidden = 64\n",
    "        features = torch.nn.Linear(self.num_state, hidden)\n",
    "        model_q = torch.nn.Sequential(\n",
    "            features,\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, self.num_action),\n",
    "        ).to(self.device)\n",
    "        model_policy = torch.nn.Sequential(\n",
    "            features,\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, self.num_action),\n",
    "        ).to(self.device)\n",
    "        ApproxVBase.initialize(self, learning_rate_name=\"beta\", model=model_q)\n",
    "        ApproxPolicyBase.initialize(self, learning_rate_name=\"alpha\", model=model_policy)\n",
    "    \n",
    "    def _loop(self, episode) -> int:\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        trajectory = []\n",
    "        for i in range(1000):\n",
    "            action, log_prob = self.policy(state)\n",
    "            _state, reward, done, _ = self.env.step(action)\n",
    "            trajectory.append((state, action, reward, done, log_prob))\n",
    "            total_reward += reward!\n",
    "            state = _state\n",
    "            if done: break\n",
    "                \n",
    "        td_targets = []\n",
    "        qs = []\n",
    "        loss_policy = []\n",
    "        for t in range(len(trajectory)):\n",
    "            G = sum([self.gamma**k * reward for k, (state, action, reward, done, log_prob) in enumerate(trajectory[t:])])\n",
    "            state, action, reward, done, log_prob = trajectory[t]\n",
    "            td_error = G - self.get_q(state)[action]\n",
    "            td_targets.append(G)\n",
    "            qs.append(self.get_q(state)[action])\n",
    "            loss_policy.append(self.gamma**t * td_error * (-log_prob))\n",
    "        self.update_q(torch.Tensor(td_targets), torch.stack(qs))\n",
    "        self.update_policy(torch.stack(loss_policy).sum())\n",
    "        return total_reward\n",
    "    \n",
    "s = REINFORCE_with_baseline(env, \n",
    "               num_episodes=500,\n",
    "               policy=\"softmax_policy\",\n",
    "               alpha=0.007, \n",
    "               beta=0.001,\n",
    "               gamma=.99)\n",
    "s.train(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's really bad for table base problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: -18.80 \tother{}\n",
      "Episode 20\tAverage Score: -12.20 \tother{}\n",
      "Episode 30\tAverage Score: -12.20 \tother{}\n",
      "Episode 40\tAverage Score: -19.50 \tother{}\n",
      "Episode 50\tAverage Score: -30.40 \tother{}\n",
      "Episode 60\tAverage Score: -18.80 \tother{}\n",
      "Episode 70\tAverage Score: -6.70 \tother{}}\n",
      "Episode 80\tAverage Score: -11.70 \tother{}\n",
      "Episode 90\tAverage Score: -10.10 \tother{}\n",
      "Episode 100\tAverage Score: -9.10 \tother{}\n"
     ]
    }
   ],
   "source": [
    "env = GridworldEnv2DState()\n",
    "s = REINFORCE(env, \n",
    "               num_episodes=100,\n",
    "               policy=\"softmax_policy\",\n",
    "               epsilon=0.01, \n",
    "               alpha=0.008, \n",
    "               gamma=.9)\n",
    "s.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
