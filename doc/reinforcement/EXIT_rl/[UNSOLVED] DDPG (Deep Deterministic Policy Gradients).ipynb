{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[medium](https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b)/ [github](https://github.com/thechrisyoon08/Reinforcement-learning/tree/master/DDPG)<br>\n",
    "[paper](https://arxiv.org/pdf/1509.02971.pdf)<br>\n",
    "[D4PG](https://github.com/msinto93/D4PG) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- this repo they add batchnorm [link](https://github.com/floodsung/DDPG/blob/master/critic_network_bn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import random\n",
    "import numpy as np\n",
    "from EXITrl.base import Base\n",
    "from EXITrl.helpers import print_weight_size, copy_params, update_params, ExperienceReplay, convert_to_tensor, device\n",
    "from EXITrl.nn_wrapper import NNWrapper\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedEnv(gym.ActionWrapper):\n",
    "    \"\"\" Wrap action \"\"\"\n",
    "\n",
    "    def _action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise(object):\n",
    "    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_space.shape[0]\n",
    "        self.low          = action_space.low\n",
    "        self.high         = action_space.high\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def evolve_state(self):\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "    def get_action(self, action, t=0):\n",
    "        ou_state = self.evolve_state()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "        return np.clip(action + ou_state, self.low, self.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        Params state and actions are torch tensors\n",
    "        \"\"\"\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate = 3e-4):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = torch.nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Param state is a torch tensor\n",
    "        \"\"\"\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = torch.tanh(self.linear3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode 10\tAverage Score: -1642.09 \tother{}\n",
      "Episode 20\tAverage Score: -1641.29 \tother{}\n",
      "Episode 30\tAverage Score: -1626.73 \tother{}\n",
      "Episode 40\tAverage Score: -1293.75 \tother{}\n",
      "Episode 50\tAverage Score: -1057.42 \tother{}\n",
      "Episode 60\tAverage Score: -1236.38 \tother{}\n",
      "Episode 70\tAverage Score: -1182.17 \tother{}\n",
      "Episode 80\tAverage Score: -1213.19 \tother{}\n",
      "Episode 90\tAverage Score: -1251.68 \tother{}\n",
      "Episode 100\tAverage Score: -1297.96 \tother{}\n",
      "Episode 110\tAverage Score: -1265.13 \tother{}\n",
      "Episode 120\tAverage Score: -1220.77 \tother{}\n",
      "Episode 130\tAverage Score: -1555.55 \tother{}\n",
      "Episode 140\tAverage Score: -1568.29 \tother{}\n",
      "Episode 150\tAverage Score: -1566.55 \tother{}\n"
     ]
    }
   ],
   "source": [
    "class DDPG(Base):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # 1. Critic\n",
    "        self.critic = NNWrapper(\n",
    "            model=Critic(self.num_state + self.num_action, 256, self.num_action),\n",
    "            lr=self.beta\n",
    "        )\n",
    "        self.critic_target = NNWrapper(\n",
    "            model=Critic(self.num_state + self.num_action, 256, self.num_action),\n",
    "            lr=self.beta\n",
    "        )\n",
    "        copy_params(self.critic.model, self.critic_target.model)\n",
    "        \n",
    "        # 2. Actor\n",
    "        self.actor = NNWrapper(\n",
    "            model=Actor(self.num_state, 256, self.num_action),\n",
    "            lr=self.alpha\n",
    "        )\n",
    "        self.actor_target = NNWrapper(\n",
    "            model=Actor(self.num_state, 256, self.num_action),\n",
    "            lr=self.alpha\n",
    "        )\n",
    "        copy_params(self.actor.model, self.actor_target.model)\n",
    "        \n",
    "        # init\n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        self.experience_replay = ExperienceReplay(num_experience=2048, num_recall=64)\n",
    "        self.noise = OUNoise(self.env.action_space)\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        self.noise.reset()\n",
    "        for i in range(1000):\n",
    "            action = self.actor.forward(convert_to_tensor(state)).detach().numpy()\n",
    "            action = self.noise.get_action(action, i)\n",
    "            _state, reward, done, _ = self.env.step(action)\n",
    "            self.experience_replay.remember(state, action, reward, _state, done)\n",
    "            \n",
    "            batch_state, \\\n",
    "            batch_action, \\\n",
    "            batch_reward, \\\n",
    "            batch_next_state, \\\n",
    "            batch_done = self.experience_replay.recall()\n",
    "\n",
    "            batch_next_actions = self.actor_target.forward(batch_next_state)\n",
    "            next_Q = self.critic_target.forward(batch_next_state, batch_next_actions.detach())\n",
    "            Q_prime = batch_reward + self.gamma * next_Q\n",
    "            Q = self.critic.forward(batch_state, batch_action)\n",
    "            \n",
    "            critic_loss = self.mse_loss(Q_prime, Q)\n",
    "            self.critic.backprop(critic_loss)\n",
    "            actor_loss = -self.critic.forward(batch_state, self.actor.forward(batch_state)).mean()\n",
    "            self.actor.backprop(actor_loss)\n",
    "            \n",
    "            update_params(self.critic.model, self.critic_target.model, 1e-2)\n",
    "            update_params(self.actor.model, self.actor_target.model, 1e-2)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = _state\n",
    "            if done: return total_reward\n",
    "            \n",
    "    def _save(self, reward):\n",
    "        torch.save({\n",
    "            'critic': self.critic.model.state_dict(),\n",
    "            'actor': self.actor.model.state_dict(),\n",
    "        }, self.save_name)\n",
    "        \n",
    "    def _load(self):\n",
    "        checkpoint = torch.load(self.save_name, map_location=device)\n",
    "        self.critic.model.load_state_dict(checkpoint['critic'])\n",
    "        self.actor.model.load_state_dict(checkpoint['actor'])\n",
    "        \n",
    "    def play(self, num_episode=3):\n",
    "        self.critic.model.eval()\n",
    "        self.actor.model.eval()\n",
    "        super().play(num_episode)\n",
    "        \n",
    "    def policy(self, state):\n",
    "        return self.actor.forward(convert_to_tensor(state)).detach().numpy()\n",
    "\n",
    "            \n",
    "try: env.close()\n",
    "except: pass\n",
    "env = NormalizedEnv(gym.make('Pendulum-v0'))\n",
    "ddpg = DDPG(env, \n",
    "           num_episodes=150,\n",
    "           alpha=0.0001, \n",
    "           beta=0.001,\n",
    "           gamma=.99,\n",
    "           save_name=\"checkpoint/Pendulum-v0-DDPG.pth\")\n",
    "ddpg.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: <class '__main__.NormalizedEnv'> doesn't implement 'action' method. Maybe it implements deprecated '_action' method.\u001b[0m\n",
      "Episode 1\tAverage Score: -898.49\n",
      "Episode 2\tAverage Score: -1002.51\n",
      "Episode 3\tAverage Score: -927.90\n"
     ]
    }
   ],
   "source": [
    "ddpg.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
